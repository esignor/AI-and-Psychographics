{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoZ7xejwoX4j"
   },
   "source": [
    "# BAG-OF-WORDS DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5Kw0aN_y5lG"
   },
   "source": [
    "### SPECIFICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npMCRhFPywVr"
   },
   "source": [
    "This script is responsable of generate BAG-OF-WORDS DATASET and the directories DATASET that contains all datasets of this project and RESULTS that contains all models results (i.e., classification and prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQitYL0MlXtJ"
   },
   "source": [
    "### USER OCCUPATIONAL CLASS THROUGH TWITTER CONTENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "teg-dyNBb4iq"
   },
   "source": [
    "Twitter Occupation Dataset, `D. Preotiuc-Pietro, V. Lampos, N. Aletras, 2015. An analysis of the user occupational class through Twitter content, 10.3115/v1/P15-1169`, I used it for generate BAG-OF-WORDS DATASET. \n",
    "\n",
    "Twitter Occupation Dataset consists of the following files:\n",
    "* `jobs-tweetids`, each line represents a tweet\n",
    "* `jobs-unigrams`, Bag-of-words unigram feature representation, one user/line\n",
    "* `dictionary`, mapping between word ids and words\n",
    "* `jobs-users`, resolved 3-digit SOC code (UK) for each user\n",
    "* `keywords`, 3-digit SOC code (UK), its corresponding class description and the keyphrases for jobs in this category used for identifying users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CB25mC5q9kU1"
   },
   "source": [
    "### CONFIGURATION\n",
    "\n",
    "#### Libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 732,
     "status": "ok",
     "timestamp": 1670439596201,
     "user": {
      "displayName": "Eleonora Signor",
      "userId": "04561396956553626752"
     },
     "user_tz": -60
    },
    "id": "37VxkeNucIPr"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-16e681696da1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m \u001b[0;31m# if pandas non exist, command line: pip3 install pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd # if pandas non exist, command line: pip3 install pandas\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3KYLoI0i9oRu"
   },
   "source": [
    "#### Project Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 820,
     "status": "ok",
     "timestamp": 1670439628708,
     "user": {
      "displayName": "Eleonora Signor",
      "userId": "04561396956553626752"
     },
     "user_tz": -60
    },
    "id": "D04a2cAfuqnd",
    "outputId": "3cd24c68-50e4-4dcf-c38a-e9bc57288c05"
   },
   "outputs": [],
   "source": [
    "%cd \"/home/eleonora/Desktop/TESI Code\"\n",
    "!ls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVJtJyUhVKZB"
   },
   "source": [
    "### TWITTER OCCUPATION DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T98Xeh3K94wS"
   },
   "source": [
    "* I am interested in identifying the profile of a Twitter user. One choice that seems suitable for this purpose is to use each user's Bag-of-words. BOWs are defined in `jobs-unigrams` constructed as: <center>``` user_id[SPACE]wordid_1:frequency_1[SPACE]...wordid_n:frequency_n ``` </center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2055,
     "status": "ok",
     "timestamp": 1670439630758,
     "user": {
      "displayName": "Eleonora Signor",
      "userId": "04561396956553626752"
     },
     "user_tz": -60
    },
    "id": "DrKBFRh3wSXK",
    "outputId": "a14e4641-ec2a-4829-a329-17408e8136b5"
   },
   "outputs": [],
   "source": [
    "with open(\"./TWITTER OCCUPATION DATASET/jobs/jobs-unigrams\", \"r\") as f: # first line of 'jobs-unigrams'\n",
    "  lines = f.read()\n",
    "  first = lines.split('\\n', 1)[0]\n",
    "\n",
    "print(first)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdDgSsLV_M7q"
   },
   "source": [
    "> *Example: user 206749819 in his Twitter posts made use of wordid 5 with frequency 2 (5:2), wordid 54 with frequency 1 (54:1), wordid 55 with frequency 2 (55:2), and so on.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUH0i-thCrKI"
   },
   "source": [
    "* Mapping between words ids and words in `dictionary` : <center>``` wordid[SPACE]word ``` </center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 320,
     "status": "ok",
     "timestamp": 1670439631075,
     "user": {
      "displayName": "Eleonora Signor",
      "userId": "04561396956553626752"
     },
     "user_tz": -60
    },
    "id": "ZiJ69ES7D9hv",
    "outputId": "392869d8-0e7f-4da9-b961-5880d4326bcd"
   },
   "outputs": [],
   "source": [
    "with open(\"./TWITTER OCCUPATION DATASET/jobs/dictionary\", \"r\") as f:  # first lines of 'dictionary'\n",
    "  lines = f.readlines()\n",
    "  i = 0\n",
    "  for line in lines:\n",
    "      print(line)\n",
    "      if i == 4: break\n",
    "      else: i += 1\n",
    "  f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrsEhC84YEb9"
   },
   "source": [
    "### BAG-OF-WORDS DATASET\n",
    "I need to proceed with creating the data to feed to the **[87] and [2], [4]** models as follows:\n",
    "* each word_id should be replaced by the corresponding word in `dictionary`\n",
    "* for each user in `jobs-unigrams` must for each word_id be expressed as defined in `GENDER Model` (i.e., $x_{word} = \\large{\\frac{freq(word, user)}{freq(\\ast, user)}}$)\n",
    "* The **BOWs DATASET** consists of the fields: key; username; [\"word\" : x_word : freq] $\\forall$ word_id of user in `jobs-unigrams`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_PIIONwxBWQ"
   },
   "source": [
    "#### FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1670439631076,
     "user": {
      "displayName": "Eleonora Signor",
      "userId": "04561396956553626752"
     },
     "user_tz": -60
    },
    "id": "mGi9v73gLyNg"
   },
   "outputs": [],
   "source": [
    "# method responsible for creating the directory DATASETS and RESULTS \n",
    "\n",
    "def create_dir(path_dir):\n",
    "    try:\n",
    "        if os.path.isdir(path_dir): # the dir already exists remove it with its contents\n",
    "            shutil.rmtree(path_dir)\n",
    "            # create the directory \"BAG-OF-WORDS DATASET\"\n",
    "            os.makedirs(path_dir)\n",
    "            print(\"Directory '% s' created successfully\" % path_dir)\n",
    "            \n",
    "    except OSError as error:\n",
    "        print(\"Directory '% s' can not be created\" % path_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1670439631077,
     "user": {
      "displayName": "Eleonora Signor",
      "userId": "04561396956553626752"
     },
     "user_tz": -60
    },
    "id": "_UF9dVrn-5SH"
   },
   "outputs": [],
   "source": [
    "# method that return content of 'dictionary'\n",
    "\n",
    "def extract_dictionary():\n",
    "    dictio = open(\"./TWITTER OCCUPATION DATASET/jobs/dictionary\", \"r\")\n",
    "    dictionaryLines = dictio.readlines()\n",
    "    dictio.close()\n",
    "\n",
    "    return dictionaryLines   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method that return content of 'jobs-unigrams'\n",
    "\n",
    "def extract_unigrams():\n",
    "    unigrams = open(\"./TWITTER OCCUPATION DATASET/jobs/jobs-unigrams\", \"r\") \n",
    "    unigramsLines = unigrams.readlines()\n",
    "    unigrams.close()\n",
    "    \n",
    "    return unigramsLines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1670439631078,
     "user": {
      "displayName": "Eleonora Signor",
      "userId": "04561396956553626752"
     },
     "user_tz": -60
    },
    "id": "1HM1gXEIV64i"
   },
   "outputs": [],
   "source": [
    "# method that does the mapping between id_word and word (in dictionary.txt)\n",
    "\n",
    "def fromWord_idToWord(id_word, dictionaryLines):\n",
    "    line = dictionaryLines[id_word-1] # id_word - 1 because dictionary start to 1 index, instead dictionaryLines from 0\n",
    "    dic = line.split(\" \")\n",
    "    return dic[1].strip('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1670439631079,
     "user": {
      "displayName": "Eleonora Signor",
      "userId": "04561396956553626752"
     },
     "user_tz": -60
    },
    "id": "7lv5nnTClOSx"
   },
   "outputs": [],
   "source": [
    "# method that calculates freq(*, user) for user\n",
    "\n",
    "def freqTotUser(items, countAllWords):\n",
    "    freq_tot = 0\n",
    "    for it in range(1, countAllWords+1): # items[1],..,items[countAllWords]\n",
    "        id_wordAndfreq = items[it].split(':')\n",
    "        freq_tot += int(id_wordAndfreq[1])\n",
    "\n",
    "    return freq_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1670439631080,
     "user": {
      "displayName": "Eleonora Signor",
      "userId": "04561396956553626752"
     },
     "user_tz": -60
    },
    "id": "Cg1bmiVV598E"
   },
   "outputs": [],
   "source": [
    "# main method that handles the creation of dframe BOW-OF-WORDS, for each user\n",
    "\n",
    "def createBAGOFWORDS_DATASET():\n",
    "\n",
    "  ## extract dictionary for have word of id_word\n",
    "  dictionaryLines  = extract_dictionary() \n",
    "\n",
    "  ## open jobs-unigrams of all users\n",
    "  unigramsLines = extract_unigrams()\n",
    "\n",
    "  df_BOWs = pd.DataFrame() # create dataframe\n",
    "  ## for each user_id/line select user_id and computation freq(*, user):\n",
    "    \n",
    "  key = 0\n",
    "  for N_user, line in enumerate(unigramsLines):\n",
    "      start = time.perf_counter()\n",
    "        \n",
    "      items = line.split() # each set of characters separated by a space \" \" is a list item: line{item[0]} = user_id\n",
    "      user_id = items[0]\n",
    "      countAllWords = len(items) - 1 # not consider items[0] because contain user_id, only words in BOW\n",
    "      freq_tot = freqTotUser(items, countAllWords) # freq(*, user)\n",
    "\n",
    "      ## computation of word and x_word per user_id:\n",
    "      data_user = \"\"\n",
    "      user_exist = False\n",
    "      for it in range(1, countAllWords+1): # items[1],..,items[countAllWords], is items is empty this cycle isn't take\n",
    "          id_wordAndfreq = items[it].split(':')\n",
    "          id_word = int(id_wordAndfreq[0])\n",
    "          freq = int(id_wordAndfreq[1]) # freq(word, user)\n",
    "          word =  str(fromWord_idToWord(id_word, dictionaryLines))\n",
    "          x_word = float(freq / freq_tot) # freq(word, user)/freq(*, user)\n",
    "\n",
    "          word_string = '\"' + word + '\"' # \"word\"   \n",
    "          data_user += word_string + \":\" + str(x_word) + \":\" + str(freq) + \" \"\n",
    "          user_exist = True\n",
    "\n",
    "      ## fill user_id, \"word\", x_word, freq in df_BOWs dataframe\n",
    "      if user_exist == True: # remove user_id which not have BOW (e.g., 101637827)\n",
    "          data_user = data_user[:-1] # remove last character (\" \")\n",
    "          print(\"yes\", N_user)\n",
    "          df = pd.DataFrame([[user_id, data_user]], columns = ['user_id', 'data'], index = [key])\n",
    "          df_BOWs = pd.concat([df_BOWs, df])\n",
    "          key += 1\n",
    "        \n",
    "      end = time.perf_counter()\n",
    "      print(user_id)\n",
    "      print(end - start)\n",
    "\n",
    "  return df_BOWs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1670439631081,
     "user": {
      "displayName": "Eleonora Signor",
      "userId": "04561396956553626752"
     },
     "user_tz": -60
    },
    "id": "YXK9EkEF3H0i"
   },
   "outputs": [],
   "source": [
    "# method that transform in 'BAG-OF-WORDS DATASET.csv' dframe dataset\n",
    "\n",
    "def outDATASET(path_dir, df_BOWs):\n",
    "    df_BOWs.to_csv(path_dir + \"BAG-OF-WORDS DATASET.csv\")\n",
    "    print(\"Successful creation of BAG-OF-WORDS DATASET.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trY6FI446vzx"
   },
   "source": [
    "### MAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4tFq83yxKpD"
   },
   "source": [
    "BOW DATASET generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oeOBIdTO6uwb",
    "outputId": "0027d2eb-2b6b-4ddc-9adf-15f35b618ecd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "path_dir_datasets = \"./DATASETS/\" # directory that contains all project datasets\n",
    "path_dir_results = \"./RESULTS/\" # directory that contains all project models results\n",
    "create_dir(path_dir_datasets)\n",
    "create_dir(path_dir_results)\n",
    "df_BOWs = createBAGOFWORDS_DATASET() # function that create datasets\n",
    "\n",
    "startcsv = time.perf_counter()\n",
    "outDATASET(path_dir_datasets, df_BOWs)       \n",
    "endcsv = time.perf_counter()\n",
    "end = time.perf_counter()\n",
    "print(\"tempo csv\", endcsv - startcsv)\n",
    "print(\"Execution time: \", end - start) \n",
    "\n",
    "\n",
    "### open file and computation is O(1); |user_id|is the number users; N_words number size BOW of each user\n",
    "### calculation of efficiency: 1 extract_dictionary (open) + 1 unigrams (open) +  x |user_id| for line_unigrams ((N_words for freq_tot + N_words + 1 for id_word + 1 for freq + 1 for word \n",
    "# + 1 for x_word) + 1 for dataframe + 1 for outDATASET = \n",
    "### = 1 + 1 + (N_words + N_words) x |user_id| + N_words x |user_id| + 1 = 2N_words x |user_id| = O(2 x N_words x |user_id|)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMETR73ySHQojDkHMLpRsYk",
   "mount_file_id": "1rXPcs7-_hMQwrUDKjy2uEkqrrMcFCnaI",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
